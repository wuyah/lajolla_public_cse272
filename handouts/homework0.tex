\documentclass{article}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\geometry{margin=1in}
\pdfminorversion=6

\newcommand\TODO[1]{\textcolor{red}{TODO: #1}}

\newcommand\header[2]{
    \begin{center}
        {\large
        UCSD CSE 272 Assignment #1: \\
        \vspace{0.3cm}
        \Large
        #2}
    \end{center}
}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
        aboveskip=3mm,
        belowskip=3mm,
        showstringspaces=false,
        columns=flexible,
        basicstyle={\small\ttfamily},
        numbers=none,
        numberstyle=\tiny\color{gray},
        keywordstyle=\color{blue},
        commentstyle=\color{dkgreen},
        stringstyle=\color{mauve},
        breaklines=true,
        breakatwhitespace=true,
        tabsize=2
}

\begin{document}

\header{0}{Introduction to the \textbf{lajolla} Renderer}

For most parts of this course, we will use a custom physically-based renderer called \textbf{lajolla}.\footnote{Many renderers are named over a location. For example, Weta Digital's renderer \emph{Manuka}'s name comes from the Manuka street in front of Weta digital's main site. Pixar's rendering algorithm Reyes comes from Point Reyes in California.} This document presents the design of the lajolla renderer. Your first homework is to read through this document, and build and run the renderer, and look at the renderer's code. This homework is not graded. However, since lajolla is still at its early stage, we expect it to have a few bugs. If you find or fix any bugs in lajolla throughout the course, you will get extra points!

Lajolla is a \emph{physically-based} renderer. It takes a 3D scene description (camera, lights, geometry, materials, etc) and input, and produces an image by simulating how photons emitted from the light sources scattered in the scene and eventually reach the camera. This is how modern visual effects, and many video games produce stunning and realistic images. Apart from movies and games, physically-based rendering is also used in augmented reality, architectural visualization, daylight simulation, product visualization, medical imaging, computer vision, autonomous driving and more. A startup \href{https://www.luxion.com/}{Luxion} from a UCSD faculty \href{http://graphics.ucsd.edu/~henrik/}{Henrik Jensen} is about building physically-based renderers and use them for many applications above.

\section{Building and running lajolla}
\begin{figure}[h]
    \includegraphics[width=0.24\linewidth]{imgs/cbox.png}
    \includegraphics[width=0.24\linewidth]{imgs/sponza.png}
    \includegraphics[width=0.24\linewidth]{imgs/veach_mis.png}
    \includegraphics[width=0.24\linewidth]{imgs/matpreview.png}
    \caption{Images you can render using lajolla.}
    \label{fig:gallery}
\end{figure}

Before we started, let's try to clone, build the code and run it. Lajolla does not require you to download any external libraries (hopefully). While we do rely on many 3rdparty libraries, we only use lightweight header-only ones that can be easily included and do not complicate the build systems. To clone the codebase, do

\begin{lstlisting}[language=bash]
  git clone https://github.com/BachiLi/lajolla_public
\end{lstlisting}

We use \href{https://cmake.org/}{CMake} as our build system. To build, on a Unix-like system, do the following from the \lstinline{lajolla_public} directory:

\begin{lstlisting}[language=bash]
  mkdir build
  cd build
  cmake ..
  make -j
\end{lstlisting}

On Windows, latest visual studio supports directly using CMake as project files.\footnote{\url{https://docs.microsoft.com/en-us/cpp/build/cmake-projects-in-visual-studio?view=msvc-170}} Alternatively, you can use CMake's graphical user interface downloaded from CMake's website. However, we have received reports that lajolla can crash on Windows machines. You might want to consider using Windows Subsystems for Linux to use a Unix-like systems for building and running lajolla (or you can help us fixing issues on Windows and get extra points!). 

Once lajolla is built, you can try to render some images in the \lstinline{scenes} directory. Try
\begin{lstlisting}[language=bash]
  ./lajolla ../scenes/cbox/cbox.xml
\end{lstlisting}

After the reading is done, you will see an image file \lstinline{image.exr} appeared in your working directory. EXRs are \emph{high-dynamic range images}\footnote{\url{https://en.wikipedia.org/wiki/High_dynamic_range}} and require a different image viewer. We recommend viewing it using HDRView\footnote{\url{https://github.com/wkjarosz/hdrview}} or Tev\footnote{\url{https://github.com/Tom94/tev}}. You should be able to see images like the ones in Figure~\ref{fig:gallery} (maybe noiser, since the default settings for these scenes use a smaller number of light simulation samples compared to the ones in the figure).

The scene format in lajolla follows the Mitsuba scene format, where you can read the documentation \href{https://www.mitsuba-renderer.org/releases/current/documentation.pdf}{here}. The scene files are human readable, so by just looking at those files should gives you a reasonable idea.

\section{Design philosophy}

Lajolla mostly follows the design of other existing physically-based renderers, such as \href{https://github.com/mmp/pbrt-v3/}{pbrt} or \href{http://www.mitsuba-renderer.org/}{Mitsuba}. It however follows a few different designs:

\paragraph{Minimalism.} While already sophisticated, lajolla is much less feature complete compared to pbrt or Mitsuba. This is the main reason we are inventing our own renderer instead of adopting existing ones. This is mostly for educational purpose: if all the features are implemented, there won't be any homeworks. : )

\paragraph{Clarity and simplicity over performance.} Lajolla does not try to become the fastest renderer in the world. The focus is on education. For example, by default lajolla uses double for most floating point computation and does not explicitly vectorize over any computation.

\paragraph{No singular light sources or materials.} Lajolla does not support point light sources or purely specular materials. These components introduce many corner cases of a renderer and require special handling, making the renderer maintenance more complicated. We approximate them using small light sources or very sharp glossy materials.

\paragraph{Portability to GPU.} Lajolla does not run on GPUs. The goal of this course is to teach the foundation of modern physically-based rendering instead of all the implementation details. That being said, given that GPU rendering is going to be increasingly important even in the offline rendering world, we need to learn how to build a renderer that is more portable to GPUs. Most functions and data structures in lajolla should be directly usable in CUDA or OpenCL code, given small amount of modification. Lajolla avoids deeply nested hierarchical data structures and also avoids heap allocation in the inner loop as much as possible.

\paragraph{Modern C++.} Lajolla uses modern C++ features extensively (fortunately, it does not use template metaprogramming extensively). This means it prefers value semantics over reference semantics:
\begin{lstlisting}[language=C++]
  std::tuple<int, int> foo(int x); // preferred
  void bar(int x, int *y, int *z); // less preferred
  std::optional<float> foo2(const float &x); // preferred
  bool bar2(const float &x, float *out); // less preferred
\end{lstlisting}
We do not declare \lstinline{auto} that often, since I am still not convinced that it would not hurt code readability.

\paragraph{Variant-based polymorphism.} Perhaps the biggest different on the surface between lajolla and most other (CPU-based) renderers is how it implements polymorphism. In rendering, the same rendering code would need to handle different materials, lights, cameras, and etc. Most renderers adopt a heavily object-oriented design with classes and inheritance. While this is an acceptable approach, the hierarchical vtable pointers make the whole scene data structure very difficult to convert to GPU. I also personally just do not think OOP is a good idea\footnote{see "Object-Oriented Programming is Bad" from Brain Will \url{https://www.youtube.com/watch?v=QM1iUe6IofM}}. To support polymorphism, we use a modern C++ feature \lstinline{std::variant}. It is essentially a \emph{tagged union} (or a \emph{sum type} if you are functional guru): a C union with a member indicating which type it is. Read more about it \href{https://www.cppstories.com/2020/04/variant-virtual-polymorphism.html/}{here}. Another renderer \href{visionaray}{https://github.com/szellmann/visionaray} also adopts a similar approach.

Below we will take a whirlwind tour over the basics of physically-based rendering and how they are implemented in lajolla.

\section{Rendering equation(s)}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{imgs/pixel_filter.pdf}
    \includegraphics[width=0.4\linewidth]{imgs/antialiasing.pdf}
    \caption{To reconstruct the continuous scene from the discrete pixel samples, the color of a pixel is reconstructed by integrating over a \emph{pixel filter}.}
    \label{fig:pixel_filter}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{imgs/camera.pdf}
    \caption{Camera configuration and pixel filtering.}
    \label{fig:camera}
\end{figure}

How does lajolla, or many other modern physically-based renderer compute the color for each pixel? They do this by \emph{integrating} all lights that goes through a pixel. Note that a pixel is not a infinitesimal point, not is it a little square~\cite{Smith:1995:PLS}. \textbf{A pixel is a reconstruction filter over an image function at a certain locatio} (Figure~\ref{fig:pixel_filter}). Mathematically, a pixel $I_{x, y}$'s color is written as an filtering integral:
\begin{equation}
    I_{x, y} = \iint_{D} k(x', y') L(\mathbf{p}, \mathbf{d}(x + x', y + y')) \mathrm{d}x\mathrm{d}y,
    \label{eq:pixel_filter}
\end{equation}
where $k$ is the \emph{filter kernel}, $D$ is the \emph{support} of the filter kernel (the pixel location where its value is non-zero), and $L(\mathbf{p}, \mathbf{d})$ is the radiance at point $\mathbf{p}$, received from direction $\mathbf{\omega}$ where the direction is pointing outwards from $\mathbf{p}$. In our case, $\mathbf{p}$ is the position of the camera, and $\mathbf{\omega}$ is the direction between the camera position and the points on the film (Figure~\ref{fig:camera}).

Now we have three questions: 1) what is the filter kernel $k$? 2) what is the radiance $L$?, and 3) how do we evaluate this integral on a computer? For 1), we will discuss a bit more in Section~\ref{sec:pixel_filter}, but for now you can think of it as something like a 2D Gaussian. For 3), these integrals typically do not have closed-form solutions,\footnote{There are rendering algorithms that based on closed-form solutions of these integrals. Some radiosity algorithms~\cite{Schroder:1993:OFF} make use of closed-form solutions, for example. Also check out the excellent paper from Heitz et al.~\cite{Heitz:2016:RPS}} so we need to \emph{discretize} these integrals:
\begin{equation}
    \int_{D} F(x) dx \approx \frac{1}{N} \sum_{i=1}^{N} F(x_i),
    \label{eq:discretization}
\end{equation}
where $x_i$ are samples in uniform measures drawn from the domain $D$. When $x_i$ are random samples, this is called \emph{Monte Carlo} integration.\footnote{Monte Carlo integration was first invented for numerical simulation of nuclear weapons. Nicholas Metropolis, a member of the Manhatton Project, suggested the name to the simulation guy Stanislaw Ulam since Ulam's uncle likes to go to Monte Carlo for gambling.} However, $x_i$ do not need to be random. It can be determinisic samples that just somewhat uniformly cover the domain. When $x_i$ are deterministic, this approximation is often called \emph{quadrature} or \emph{cubature}. There is another class of deterministic integration methods called \emph{Quasi Monte Carlo} methods, which is based on the idea of carefully placing samples to make sure they are not far away from each other. We will cover these in the class if time permits.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{imgs/pathtracing.pdf}
    \caption{The illustration of the rendering equation.}
    \label{fig:pathtracing}
\end{figure}

We are still left with Q2: what is the radiance $L$? In this class, we are mostly dealing with objects that are larger than the wavelength of lights. So under Newtonian optics, the radiance is simply the radiance $L_i$ at the object we see:
\begin{equation}
    L(\mathbf{p}, \mathbf{\omega}) = L_i(\mathbf{p}', -\mathbf{\omega}),
\end{equation}
where $\mathbf{p}'$ is the point the direction $\mathbf{d}$ intersect with the scene. Then -- what is $L_i$? An object is lit by a light source. However, as photons scatter between objects, a photon can first hit object A before it hits object B. Therefore we need to recursively consider all objects in the scene:
\begin{equation}
    L_i(\mathbf{p}, \mathbf{\omega}) = L_e(\mathbf{p}, \mathbf{\omega}) + \int_{\Omega} f(\mathbf{p}, \mathbf{\omega}, \mathbf{\omega}') |\mathbf{n}_{\mathbf{p}} \cdot \mathbf{\omega}'| L_i(\mathbf{p}', -\mathbf{\omega}') \mathrm{d} \mathbf{\omega},
    \label{eq:rendering_equation}
\end{equation}
where $L_e$ is the \emph{emission} at $\mathbf{p}$ towards direction $\mathbf{\omega}$, $\Omega$ is a spherical domain around point $\mathbf{p}$, $f$ is the reflectance of the object at point $p$ from direction $\mathbf{\omega}'$ to $\mathbf{\omega}$, $\mathbf{p}'$ is the point intersected by the ray $(\mathbf{p}, \mathbf{\omega}')$, and $n_{p}$. $f$ is often called the Bidirectional Scattering Distribution Function (BSDF) -- when the material is transmissive, it's called the Bidirectional Transmission Distribution Function (BTDF), and when it is reflective, it's called the Bidirectional Reflectance Distribution Function (BRDF) (I know, it's confusing). See Figure~\ref{fig:pathtracing} for an illustration. The measure $\mathrm{d} \mathbf{\omega}_{\bot}'$ is the \emph{solid angle}: the infinitesimal area on a unit sphere. Equation~\ref{eq:rendering_equation} is often called the \emph{rendering equation}~\cite{Kajiya:1986:RE}.

This recursive integral, combined with the integral discretization (Equation~\ref{eq:discretization}), gives us an rendering algorithm. We start from a pixel $(x, y)$, and sample a pixel offset $(x', y')$ to estimate the pixel filter integral (Equation~\ref{eq:pixel_filter}). Next, to estimate $L_i$, we sample a direction $\mathrm{d}'$, hit point $\mathbf{p}'$, multiply by $f$, and add $L_e$ whenever we hit a light source. In lajolla, the \lstinline{path_tracing} function in \lstinline{path_tracing.h} implements a slightly more sophisticated version of this. In below we describe those improvements.

\subsection{Importance sampling}
The naive algorithm above is correct, but can be inefficient. Consider a special case of the rendering equation, where we only recurse once:
\begin{equation}
    L_i(\mathbf{p}, \mathbf{d}) = L_e(\mathbf{p}, \mathbf{\omega}) + \int_{\Omega} f(\mathbf{p}, \mathbf{\omega}, \mathbf{\omega}') |\mathbf{n}_{\mathbf{p}} \cdot \mathbf{\omega}'| L_e(\mathbf{p}', -\mathbf{\omega}') \mathrm{d} \mathbf{\omega}.
\end{equation}
The only difference is that instead of $L_i$ in the integral that recursively integrate over all the other surfaces, we only consider the light source contribution $L_e$. Remember that we approximate the integral using a discrete sum. Suppose we randomly sample a direction on a sphere:
\begin{equation}
  \int_{\Omega} f(\mathbf{p}, \mathbf{\omega}, \mathbf{\omega}') |\mathbf{n}_{\mathbf{p}} \cdot \mathbf{\omega}'| L_e(\mathbf{p}', -\mathbf{\omega}') \mathrm{d} \mathbf{\omega} \approx \frac{1}{N} \frac{1}{\frac{1}{4 \pi}} \sum_{i}^{N} f(\mathbf{p}, \mathbf{\omega}, \mathbf{\omega}_i') |\mathbf{n}_{\mathbf{p}} \cdot \mathbf{\omega}'| L_e(\mathbf{p}', -\mathbf{\omega}_i').
\end{equation}
The $\frac{1}{\frac{1}{4\pi}}$ factor accounts for the uniform measure on an unit sphere. The variance of this sum can be really high if $N$ is not large enough: firstly, not every surface in the scene emits light, so only a small fraction of samples would have non-zero $L_e$. Secondly, our sampling does not take the BSDF $f$ into consideration: $f$ can have high variation between different samples.

Fortunately, there is a way to make this sampling more efficient. The key is to apply a change of variable to our integral. Let's focus on the emission $L_e$ first. Instead of blindly sampling a direction on the sphere, we can directly pick a point on the light source, then choose a direction towards that point. So instead of integrating over the direction $\mathbf{\omega}$, we apply a change of variable to instead integrate over the points $\mathbf{p}$ that emit lights:
\begin{equation}
    \int_{\Omega} f(\mathbf{p}, \mathbf{\omega}, \mathbf{\omega}') |\mathbf{n}_{\mathbf{p}} \cdot \mathbf{\omega}'| L_e(\mathbf{p}', -\mathbf{\omega}') \mathrm{d} \mathbf{\omega} =
    \int_{E} f(\mathbf{p}, \mathbf{\omega}, \mathbf{\omega}'(\mathbf{p}')) |\mathbf{n}_{\mathbf{p}} \cdot \mathbf{\omega}'| L_e(\mathbf{p}', -\mathbf{\omega}') G(\mathbf{p}, \mathbf{p}') \mathrm{d} \mathbf{p}',
\end{equation}
where $E$ is the area of all light sources, and $G$ is the Jacobian of the variable reparametrization (often called the \emph{geometry term}). Using some geometry insights, we can derive that $G(\mathbf{p}, \mathbf{p}') = \frac{|\mathbf{n}_{\mathbf{p}'} \cdot \mathbf{d}'|}{\|\mathbf{p} - \mathbf{p}'\|^2} V(\mathbf{p}, \mathbf{p}')$, where $V(\mathbf{p}, \mathbf{p}')$ is the \emph{visibility function} where $V=1$ if $\mathbf{p}$ can see $\mathbf{p}'$ and $V=1$ if they are occluded. Intuitively, $G$ is the area ratio of an infinitesimal area projected onto an unit sphere.

When the light source is visible for most of the time, the discretization of the second integral is much more efficient at locating lights since we focus on the domain $E$ instead of the whole spherical domain.

We can similarly account for the BSDF $f$ and the cosine $|\mathbf{n}_{\mathbf{p}} \cdot \mathbf{\omega}'|$ using a similar change of variable trick. This change of variable is often called \emph{importance sampling}.

In general, for a (potentially multi-dimensional) integral $\int g(x) \mathrm{d}x$, we can rewrite it using a change of variable $y = T(x)$, where $T$ is an invertible function. This gives us $\int g(T^{-1}(y)) \frac{1}{|\frac{\mathrm{d}y}{\mathrm{d}x}|} \mathrm{d}y$, where $|\frac{\mathrm{d}y}{\mathrm{d}x}|$ is the Jacobian of the mapping $T$. If we discretize the new integral using uniform samples $y_i$, then we can see the new discretization as drawing samples $x_i = T^{-1}(y_i)$, evaluate $g(x_i)$, then weight them with the inverse of the Jacobian $|\frac{\mathrm{d}y}{\mathrm{d}x}|$ (as opposed to drawing $x_i$ as uniform samples and weight them with $1$). The Jacobian can be seen as the \emph{probability density function} of the samples $x_i = T^{-1}(y_i)$ (since the probability density function needs to integrate to $1$, we are essentially applying a change of variable for that integral too). 

In lajolla, the \lstinline{sample_light} and \lstinline{sample_point_on_light} functions implement the light source sampling strategy above. \lstinline{sample_light} pick a light source (from many) proportional to their relative powers, and \lstinline{sample_point_on_light} pick a point on the selected light source. \lstinline{light_pmf} and \lstinline{pdf_point_on_light} are the probability density (or probability mass function in the case of discrete distributions) of the corresponding sampling operations. Similarly, \lstinline{sample_bsdf} samples a direction that is proportional to $f$, and \lstinline{pdf_sample_bsdf} is the corresponding probability density.

\subsection{Multiple importance sampling}

If we have multiple change of variables we want to apply (e.g., sampling from lights and sampling from BSDFs), how do we combine them? One (inefficient) way to do this is to combine them by averaging the results. For example, for an integral $\int g(x) \mathrm{d}x$, we can apply $y = T_1(x)$ and $z = T_2(x)$, and evaluate $\frac{g(x(y_i))}{
\frac{\mathrm{d}T_1}{\mathrm{d}x}|}$ and $\frac{g(x(z_i))}{|\frac{\mathrm{d}T_2}{\mathrm{d}x}|}$. We can approximate the integral as
\begin{equation}
\frac{1}{N} \sum
\frac{1}{2} \frac{g(x(y_i))}{|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(y_i))|} + 
\frac{1}{2} \frac{g(x(z_i))}{|\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(z_i))|}.
\end{equation}

This is, however, not a great idea. If one of our sampling transformation has high variance, even if the other one achieves zero variance, this resulting estimator would still have very high variance.

Eric Veach made a breakthrough in physically-based rendering by figuring out how we combine different estimators~\cite{Veach:1995:OCS}. The idea is to apply higher weight for the \emph{better} estimator. How do we know which estimator is better? We look at the Jacobian $|\frac{\mathrm{d}T}{\mathrm{d}x}(x)|$, or equivalently the probability density: if the Jacobian is high, that means that our sampling strategy has high sampling density around $x_i$ -- which means it's more efficient around that area. We can weight the samples using the corresponding Jacobian:
\begin{equation}
\frac{1}{N} \sum
\frac{|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(y_i))|}{|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(y_i))| + |\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(y_i))|}
\frac{g(x(y_i))}{|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(y_i))|} + 
\frac{|\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(z_i))|}{|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(z_i))| + |\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(z_i))|}
\frac{g(x(z_i))}{|\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(z_i))|}.
\end{equation}
Note that there are four different Jacobians here: $|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(y_i))|$, $|\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(y_i))|$, $|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(z_i))|$, and $|\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(z_i))|$.

Alternatively, Veach also proposed another weight called the \emph{power heuristics} that put even more emphasis on strategies with high Jacobian values:
\begin{equation}
\frac{1}{N} \sum
\frac{|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(y_i))|^2}{|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(y_i))|^2 + |\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(y_i))|^2}
\frac{g(x(y_i))}{|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(y_i))|} + 
\frac{|\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(z_i))|^2}{|\frac{\mathrm{d}T_1}{\mathrm{d}x}(x(z_i))|^2 + |\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(z_i))|^2}
\frac{g(x(z_i))}{|\frac{\mathrm{d}T_2}{\mathrm{d}x}(x(z_i))|}.
\end{equation}

Veach made more justification of the method in his paper (which I highly recommend you to read!). However note that these are ultimately ``heuristics'', and they can be suboptimal in certain cases. Finding better multiple importance sampling weights is an active research area.

lajolla uses the power heuristics for combining different strategies.

\paragraph{The path-space integral.} The rendering equation above is a recursive equation, and while recursion is cool, it's often easier to analyze and understand its structure if we can flatten it to a more iterative form. By expanding the recursion, we see the following pattern
\begin{equation}
    L_i = L_e + f * L_e + f * f * L_e + \dots.
\end{equation}
We notice that we are essentially tracing \emph{light paths} with different number of vertices. For light paths with two vertices: one at the camera position and one at a light source, we only need to compute $L_e$. For light paths with three vertices, we need to account for one scattering event and multiply $f$. If we define a light path with a sequence of directions: $\mathbf{D} = ((x', y'), \mathbf{d}_1, \mathbf{d}_2, \dots)$, then we can define a \emph{path contribution function} $f_d$ to compute the contribution of that single light path. From above, we can see that
\begin{equation}
    f_d(\mathbf{D}) = k(x', y') f(\mathbf{p}_1, \mathbf{d}_0(x', y'), \mathbf{d}_1) f(\mathbf{p}_2, \mathbf{d}_1, \mathbf{d}_2) \dots L_e(\mathbf{p}_N, \mathbf{d}_N).
\end{equation}
(I might have get some signs wrong but don't worry about it.)

We can then rewrite the rendering integral as
\begin{equation}
    I_{x, y} = \int f_d(\mathbf{D}) \mathrm{d} D(\mathbf{d}),
\end{equation}
where the measure $D(\mathbf{d})$ is the product of solid angle of the path directions $\mathbf{d}_i$.

A path can also be written in many different forms. A very convienent form is to define the path as a sequence of positions, as opposed of directions: $\mathbf{P} = (\mathbf{p}_0, \mathbf{p}_1, \mathbf{p}_2, \dots)$. While these two forms are equivalent, this again is a change of variable. The new path contribution function $f_p(\mathbf{P})$ is:
\begin{equation}
    f_p(\mathbf{P}) = k((x', y')(\mathbf{p}_0, \mathbf{p_1})) G_0(\mathbf{p}_0, \mathbf{p}_1) f(\mathbf{p}_1, \mathbf{d}_0, \mathbf{d}_1)G(\mathbf{p}_1, \mathbf{p}_2) f(\mathbf{p}_2, \mathbf{d}_1, \mathbf{d}_2) \dots G(\mathbf{p}_{N-1}, \mathbf{p}_{N}) L_e(\mathbf{p}_N, \mathbf{d}_N),
\end{equation}
where $G$ is again the geometry term that accounts for the Jacobian. The first geometry term $G_0$ is slightly more complicated as it is the Jacobian between $\mathbf{p}_1$ and pixel integral variables $x', y'$. We don't need it here yet, so don't worry about it.

This path integral enables a more principled way to think about rendering algorithms. For example, we do not need to always trace paths from the cameras. We can trace paths from lights and it still gives us a valid light path. We can even just randomly sample spatial location to construct a sequence of positions $\mathbf{p}$.

\section{Camera}

Lajolla implements a fairly standard pinhole perspective camera:
\begin{lstlisting}[language=c++]
struct Camera {
    Camera() {}
    Camera(const Matrix4x4 &cam_to_world,
           Real fov, // in degree
           int width, int height,
           const Filter &filter);

    Matrix4x4 sample_to_cam, cam_to_sample;
    Matrix4x4 cam_to_world, world_to_cam;
    int width, height;
    Filter filter;
};
\end{lstlisting}
The matrices transforms a 2D pixel location $(x, y)$ to a ray direction $\omega$. The \lstinline{sample_primary} function implements the functionality for sampling a ray given a screen position.
\begin{lstlisting}[language=c++]
/// Given screen position in [0, 1] x [0, 1], generate a camera ray.
Ray sample_primary(const Camera &camera,
                   const Vector2 &screen_pos);
\end{lstlisting}

\subsection{Pixel filter}
\label{sec:pixel_filter}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/box.png}
        \caption{box}
        \label{fig:box_filter}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/tent.png}
        \caption{tent}
        \label{fig:tent_filter}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/gaussian.png}
        \caption{Gaussian ($\sigma = 0.75$)}
        \label{fig:gaussian_filter}
    \end{subfigure}
    \caption{A poor pixel filter can lead to the \emph{aliasing} artifact: high-frequency signals ``leak'' into the reconstruction and we see patterns that should not exist (zoom in and observe the top part of the images). Blurring the signal resolves this issue: the design of a good pixel filter trades between aliasing, blurriness, and ringing (oversharpening).\protect\footnotemark}
    \label{fig:pixel_filter_comp}
\end{figure}

A crucial component of the camera is the \emph{pixel filter}, which corresponds to the $k$ kernel above. As we have emphasized: \textbf{a pixel is not a little square}. It is a reconstruction from a discrete sampling of a continuous underlying signal. The choice of the reconstruction filter/kernel can have significant impact on the quality of the reconstruction. A naive box filter can suffer from servere \emph{aliasing} effects, since it's Fourier transform is a sinc function that can have negative weights in frequency domain and high-frequency energy. Lajolla currently supports three different filters (Figure~\ref{fig:pixel_filter_comp} \footnotetext{In this particular case, a proper texture prefiltering should give a better result, but checkerboard texture filtering has not been implemented in lajolla.}):

\begin{lstlisting}[language=c++]
struct Box {
    Real width;
};
struct Tent {
    Real width;
};
struct Gaussian {
    Real stddev;
};
using Filter = std::variant<Box, Tent, Gaussian>;
\end{lstlisting}

Lajolla implements pixel filtering in a slightly different way compared to other open source renderes such as Mitsuba or pbrt. Many common open source renderers implement pixel filtering using a "splatting" approach: they sample a point from a pixel, and then splat the contribution to all nearby pixels overlapped with the filter support. This approach works fine, but has a few disadvantages:
\begin{itemize}
    \item This introduces race conditions between different pixels, and require atomic operations.
    \item This introduces correlation between pixels and hurts denoising.
    \item The splatting approach is biased and creates artifacts at low sampling rates.
    \item For filters with infinite supports (e.g., Gaussian), this requires a discontinuous cutoff radius (otherwise it would be too slow).
\end{itemize}
For these reasons, many modern production renderers have started to employ a different and simpler strategy. For each pixel, we solve for the pixel filter integral by directly importance sample that filter, and we *do not* share samples among pixels. This allows us to avoid all three problems above. This approach was described by Shirley et al. in 1991~\cite{Shirley:1991:RTF} and was discussed more recently by Ernst et al.~\cite{Ernst:2006:FIS}.

\section{Materials}
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \begin{minipage}[t]{\linewidth}
            \includegraphics[width=\linewidth]{imgs/lambertian_render.png}
            \includegraphics[width=\linewidth]{imgs/lambertian.pdf}
        \end{minipage}
        \caption{Lambertian}
        \label{fig:lambertian}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \begin{minipage}[t]{\linewidth}
            \includegraphics[width=\linewidth]{imgs/roughplastic_render.png}
            \includegraphics[width=\linewidth]{imgs/roughplastic.pdf}
        \end{minipage}
        \caption{rough plastic}
        \label{fig:rough_plastic}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \begin{minipage}[t]{\linewidth}
            \includegraphics[width=\linewidth]{imgs/roughdielectric_render.png}
            \includegraphics[width=\linewidth]{imgs/roughdielectric.pdf}
        \end{minipage}
        \caption{rough dielectric}
        \label{fig:rough_dielectric}
    \end{subfigure}
    \caption{Three materials lajolla supports currently. Lambertian is a diffusive material that reflects light uniformly. Rough plastic is a two-layer material with a dielectric coating at the top, and a diffusive material at the bottom. The Fresnel equation $F$ determines how much lights are reflected, and how much are refracted into the diffusive material. If light is reflected at the dielectric layer, a distribution centered at the mirror reflection angle determined by the microfacet theory~\cite{Cook:1982:RMC} is used. If light is reflected through the dielectric layer, a Lambertian response is used, attenuated by the Fresnel reflection. Finally, the rough dielectric material is similar to the dielectric layer of rough plastic, but it refracts light by an amount predicted by Fresnel equation.}
    \label{fig:materials}
\end{figure}

Recall that in the rendering equation, we need the BSDF $f$ for describing how objects reflect or refract lights. Designing and capturing BSDFs is a large field of study, and we will cover some of them in this course. Lajolla currently supports three different materials/BSDFs (and you will implement one more in the next homework!):
\begin{lstlisting}[language=c++]
struct Lambertian {
    Texture<Spectrum> reflectance;
};

struct RoughPlastic {
    Texture<Spectrum> diffuse_reflectance;
    Texture<Spectrum> specular_reflectance;
    Texture<Real> roughness;

    // Note that the material is not transmissive.
    // This is for the IOR between the dielectric layer and the diffuse layer.
    Real eta; // internal IOR / externalIOR
};

struct RoughDielectric {
    Texture<Spectrum> specular_reflectance;
    Texture<Spectrum> specular_transmittance;
    Texture<Real> roughness;

    Real eta; // internal IOR / externalIOR
};
\end{lstlisting}

See Figure~\ref{fig:materials} for explanation of the three materials. While we will cover them in the course, I highly recommend Cook and Torrance's seminal paper~\cite{Cook:1982:RMC} for a primer of the microfacet-based BSDFs. You should also read Walter's paper on extending Cook and Torrance's model to handle refraction~\cite{Walter:2007:MMR}.

\paragraph{Material interfaces.} Every material in lajolla needs to implement the following functions:
\begin{lstlisting}[language=c++]
/// Given incoming direction and outgoing direction of lights,
/// both pointing outwards of the surface point,
/// outputs the BSDF times the cosine between outgoing direction
/// and the shading normal, evaluated at a point.
/// When the transport direction is towards the lights,
/// dir_in is the view direction, and dir_out is the light direction.
/// Vice versa.
Spectrum eval(const Material &material,
              const Vector3 &dir_in,
              const Vector3 &dir_out,
              const PathVertex &vertex,
              const TexturePool &texture_pool,
              TransportDirection dir = TransportDirection::TO_LIGHT);

struct BSDFSampleRecord {
    Vector3 dir_out;
    // The index of refraction ratio. Set to 0 if it's not a transmission event.
    Real eta;
    Real roughness; // Roughness of the selected BRDF layer ([0, 1]).
};

/// Given incoming direction pointing outwards of the surface point,
/// samples an outgoing direction. Also returns the index of refraction
/// and the roughness of the selected BSDF layer for path tracer's use.
/// failed (e.g., if the incoming direction is invalid).
/// If dir == TO_LIGHT, incoming direction is the view direction and 
/// we're sampling for the light direction. Vice versa.
std::optional<BSDFSampleRecord> sample_bsdf(
    const Material &material,
    const Vector3 &dir_in,
    const PathVertex &vertex,
    const TexturePool &texture_pool,
    const Vector2 &rnd_param_uv,
    const Real &rnd_param_w,
    TransportDirection dir = TransportDirection::TO_LIGHT);

/// Given incoming direction and outgoing direction of lights,
/// both pointing outwards of the surface point,
/// outputs the probability density of sampling.
/// If dir == TO_LIGHT, incoming direction is dir_view and 
/// we're sampling for dir_light. Vice versa.
Real pdf_sample_bsdf(const Material &material,
                     const Vector3 &dir_in,
                     const Vector3 &dir_out,
                     const PathVertex &vertex,
                     const TexturePool &texture_pool,
                     TransportDirection dir = TransportDirection::TO_LIGHT);

/// Return a texture from the material for debugging.
/// If the material contains multiple textures, return an arbitrary one.
const TextureSpectrum &get_texture(const Material &material);
\end{lstlisting}

The comments should be self-explanatory. If they are not, let us know. Note that in lajolla, the BSDF functions also contain the cosine term $|\mathbf{n}_{\mathbf{p}} \cdot \omega'|$: this makes the overall code simpler. \textbf{For the homeworks, you do not need to worry about \lstinline{TransportDirection}: you can assume they are always pointing towards the lights.} This is for convienence for implementing a bidirectional path tracer.

\subsection{Textures and filtering}
Almost all values of a material should be spatially varying. Textures are a way to describe these spatially varying materials using a 2D function: given a 2D coordinates $(u, v)$, a texture maps it to a value $T$ (can be a color or just a real number). There are three kinds of textures in lajolla currently:

\begin{lstlisting}[language=c++]
template <typename T>
struct ConstantTexture {
    T value;
};

template <typename T>
struct ImageTexture {
    int texture_id;
    Real uscale, vscale;
    Real uoffset, voffset;
};

template <typename T>
struct CheckerboardTexture {
    T color0, color1;
    Real uscale, vscale;
    Real uoffset, voffset;
};

template <typename T>
using Texture = std::variant<ConstantTexture<T>, ImageTexture<T>, CheckerboardTexture<T>>;
using Texture1 = Texture<Real>;
using TextureSpectrum = Texture<Spectrum>;

/// Evaluate the texture at location uv.
/// Footprint should be approximatedly min(du/dx, du/dy, dv/dx, dv/dy) for texture filtering.
template <typename T>
T eval(const Texture<T> &texture, const Vector2 &uv, Real footprint, const TexturePool &pool);
\end{lstlisting}

In lajolla, all surfaces must come with a 2D $(u, v)$ parametrization. We use this 2D $uv$ vector $(\in [0, 1]^2)$ for accessing the textures. A constant texture uses a constant value for all $uv$. An image texture loads an image from file and uses it for representing the texture. A checkerboard texture is a procedural texture that outputs a checkerboard pattern (see Figure~\ref{fig:pixel_filter_comp} for example).

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{imgs/texture_filtering.pdf}
    \caption{The pixel footprint will propagate to texture access, requiring us to access a large region of the texture. The size of the ellipse on the texture is determined by the derivative $\frac{\mathrm{d}u}{\mathrm{d}x}, \frac{\mathrm{d}u}{\mathrm{d}y}, \frac{\mathrm{d}v}{\mathrm{d}x},$ and $\frac{\mathrm{d}v}{\mathrm{d}y}$.}
    \label{fig:texture_filtering}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{imgs/ray_differential.pdf}
    \caption{We use a simplified and conservative ray differential adopted by RenderMan: a radius and a spread is maintain for each ray for determining the texture footprint.}
    \label{fig:ray_differential}
\end{figure}

\paragraph{Filtering and ray differentials.} Similar to the pixel filter, it is crucial that we filter the texture as well to avoid aliasing. When accessing a texture, we want to know the relative size between the filter on uv and the pixel filter (Figure~\ref{fig:texture_filtering}). This relative size is determined by the derivatives between $uv$ and $xy$. Note that we need to access textures for many bounces of lights, and it is difficult to estimate the texture footprint over many bounces. A common used method for determining texture footprint is the \emph{ray differentials}~\cite{Igehy:1999:TRD}, however it only works for purely specular materials. \emph{Path differentials}~\cite{Suykens:2001:PDA} is an extension of ray differential for glossy materials, however it is extremely expensive to compute. A slightly more principled way based on local Fourier analysis called covariance tracing~\cite{Belcour:2017:ACG} can propagate the Fourier spectrum of the pixel filter using a Gaussian approximation, but 1) it requires us to have knowledge of the Fourier spectrum of the BSDF, and 2) it is still an approximation because it ignores visibility. In general, if we ignore visibility, all existing methods would predict an infinite filter size if a light path goes through a diffuse bounce. However this is clearly incorrect, since object boundaries and visibility would limit the texture filter size. Existing approach has to place a manually determined maximum filter size to prevent infinite filter size.

Since all existing methods are heuristics and there is not really a satisfactory solution for multiple bounces (which means there are research to be done here!), we will use a very simple and conservative heuristics, inspired by the implementation in modern RenderMan~\cite{Christensen:2018:RAP} and an experimental branch of pbrt.\footnote{See \href{THE IMPLEMENTATION OF A SCALABLE TEXTURE CACHE}{https://www.pbrt.org/texcache.pdf} from Matt Pharr} We maintain two numbers for a ray differential: the \emph{radius} of a ray and a \emph{spread} of a ray (Figure~\ref{fig:ray_differential}) -- now that the ray becomes a \emph{ray cone} and the size of the cone can be used for determining the texture footprint. These values are approximatedly the derivative of the ray origin and ray direction with respect to the pixel coordinates.
\begin{lstlisting}[language=c++]
struct RayDifferential {
    // Radius is approximatedly (length(dp/dx) + length(dp/dy)) / 2
    // Spread is approximatedly (length(dd/dx) + length(dd/dy)) / 2
    // p is ray position, d is ray direction.
    Real radius = 0, spread = 0; // The units are pixels.
};
\end{lstlisting}

To initiailize a ray differential, we set it to $1/4$ the size of a pixel:
\begin{lstlisting}[language=c++]
inline RayDifferential init_ray_differential(int w, int h) {
    return RayDifferential{Real(0), Real(0.25) / max(w, h)};
}
\end{lstlisting}

To propagate a ray differential over a distance, we update the radius of the ray using its spread
\begin{lstlisting}[language=c++]
/// Update the radius (dp/dx) of a ray differential by propagating it over a distance.
inline Real transfer(const RayDifferential &r, Real dist) {
    return r.radius + r.spread * dist;
}
r.radius = transfer(r, dist);
\end{lstlisting}

To model a scattering event, we blend between the spread of a pure specular event and a spread of a diffuse event.
For a pure specular event, the amount of spread is determined by the curvature of a surface.
For the diffuse event, we manually set a cone size.
\begin{lstlisting}[language=c++]
/// Update the spread (dd/dx) of a ray differential by scattering over a reflective surface.
inline Real reflect(const RayDifferential &r,
                    Real mean_curvature,
                    Real roughness) {
    Real spec_spread = r.spread + 2 * mean_curvature * r.radius;
    Real diff_spread = Real(0.2);
    return fmax(spec_spread * (1 - roughness) + diff_spread * roughness, Real(0));
}
\end{lstlisting}

For a refraction, we need to additionally take the index of refraction into account. According to Snell-Descartes law, the ray spread would change by the ratio of index of refraction. If the ratio is large, rays will be more concentrated, and vice versa. So we just divide the spread with that ratio.
\begin{lstlisting}[language=c++]
/// Update the spread (dd/dx) of a ray differential by scattering over a reflective surface.
inline Real reflect(const RayDifferential &r,
                    Real mean_curvature,
                    Real roughness) {
    Real spec_spread = (r.spread + 2 * mean_curvature * r.radius) / eta;
    Real diff_spread = Real(0.2);
    return fmax(spec_spread * (1 - roughness) + diff_spread * roughness, Real(0));
}
\end{lstlisting}
Note that these are all approximation and heuristics. Read the references above to see other alternatives as well.

Once we have the footprint of a ray radius (approximately $\frac{dp}{dx}$), we can convert it to the UV space using the derivative $\frac{du}{dp}$, which we can compute from the geometry quantities. This uv space footprint is then passed to the texture as the \lstinline{footprint} argument. We then want to filter the texture using a filter with support of \lstinline{footprint}. Looping over all texels inside the footprint is too slow. Therefore we precompute an \emph{image pyramid} of the texture for representing signals at different frequencies. This image pyramid is called the Mipmap. We can then reconstruct a value of a texture with a certain footprint, by linearly interpolating levels between a mipmap.

\begin{lstlisting}[language=c++]
template <typename T>
struct Mipmap {
    std::vector<Image<T>> images;
};

/// Bilinear lookup of a mipmap at location (uv) with an integer level
template <typename T>
inline T lookup(const Mipmap<T> &mipmap, Real x, Real y, int level);

/// Trilinear look of of a mipmap at (u, v, level)
template <typename T>
inline T lookup(const Mipmap<T> &mipmap, Real u, Real v, Real level) {
    if (level <= 0) {
        return lookup(mipmap, u, v, 0);
    } else if (level < Real(mipmap.images.size() - 1)) {
        int flevel = std::clamp((int)floor(level), 0, (int)mipmap.images.size() - 1);
        int clevel = std::clamp(flevel + 1, 0, (int)mipmap.images.size() - 1);
        Real level_off = level - flevel;
        return lookup(mipmap, u, v, flevel) * (1 - level_off) +
               lookup(mipmap, u, v, clevel) *      level_off;
    } else {
        return lookup(mipmap, u, v, int(mipmap.images.size() - 1));
    }
}
\end{lstlisting}

To determine the mipmap level, we just take a base 2 log of the footprint.
\begin{lstlisting}[language=c++]
template <typename T>
T eval_texture_op<T>::operator()(const ImageTexture<T> &t) const {
    const Mipmap<T> &img = get_img(t, pool);
    Vector2 local_uv{modulo(uv[0] * t.uscale + t.uoffset, Real(1)),
                     modulo(uv[1] * t.vscale + t.voffset, Real(1))};
    Real scaled_footprint = max(get_width(img), get_height(img)) * max(t.uscale, t.vscale) * footprint;
    Real level = log2(max(scaled_footprint, Real(1e-8f)));
    return lookup(img, local_uv[0], local_uv[1], level);
}
\end{lstlisting}

\paragraph{Texture Pool.} Modern production renderers do not actually load all textures to the memory when parsing
the scene (since that would be up to terabytes of data). They only store the ID or the name of a texture, and load the texture whenever necessary. While a full texture caching system is too complicated and should be in lajolla, at least we can make a similar interface :p. Notice that our image texture only stores a texture ID instead of an image. You'll need to access the image using a \lstinline{TexturePool}.
\begin{lstlisting}[language=c++]
/// Can be replaced by a more advanced texture caching system,
/// where we only load images from files when necessary.
/// See OpenImageIO for example https://github.com/OpenImageIO/oiio
struct TexturePool {
    std::map<std::string, int> image1s_map;
    std::map<std::string, int> image3s_map;

    std::vector<Mipmap1> image1s;
    std::vector<Mipmap3> image3s;
};

inline int insert_image1(TexturePool &pool, const std::string &texture_name, const fs::path &filename);
inline int insert_image1(TexturePool &pool, const std::string &texture_name, const Image1 &img);
inline int insert_image3(TexturePool &pool, const std::string &texture_name, const fs::path &filename);
inline int insert_image3(TexturePool &pool, const std::string &texture_name, const Image3 &img);
inline const Mipmap1 &get_img1(const TexturePool &pool, int texture_id);
inline const Mipmap3 &get_img3(const TexturePool &pool, int texture_id);
\end{lstlisting}

I recommend Matt Pharr's article \href{The Implementation of a Scalable Texture Cache}{https://www.pbrt.org/texcache.pdf} if you want to learn more about texture caches.

That was a lot: handling textures correctly is difficult and there is actually more! We will cover some of them in the classes.

\section{Lights}
We have not yet explained how we model the emission $L_e$ in the rendering equation. There are currently two kinds of lights in lajolla: a constant area light, and an \emph{environment map}.
\begin{lstlisting}[language=c++]
/// An area light attached on a shape to make it emit lights.
struct DiffuseAreaLight {
    int shape_id;
    Vector3 intensity;
};

/// An environment map (Envmap) is an infinitely far area light source
/// that covers the whole bounding spherical domain of the scene.
/// A texture is used to represent light coming from each direction.
struct Envmap {
    Texture<Spectrum> values;
    Matrix4x4 to_world, to_local;

    // For sampling a point on the envmap
    TableDist2D sampling_dist;
};

using Light = std::variant<DiffuseAreaLight, Envmap>;
\end{lstlisting}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{imgs/envmap.pdf}
    \caption{An environment map is an infinitely far away area light the maps each point on a sphere to a directional light source. (Sorry for the abstract drawing. Hopefully you get the idea.)}
    \label{fig:envmap}
\end{figure}

A diffuse area light should be straightforward: it is attached to a geometry, and emits lights uniformly at all direction. An environment map is usually represented as an image, and emits light on an sphere that is infinitely large (Figure~\ref{fig:envmap}). These two light sources are the most common kind in production. Some renderers implement area lights with spatial-directionally varying properties, but the emission profile of light sources are more scarce compared to BSDF (which is already scarce) -- more research needed!

All lights in lajolla needs to implement the following functions:
\begin{lstlisting}[language=c++]
/// Computes the total power the light emit to all positions and directions.
/// Useful for sampling.
Real light_power(const Light &light, const Scene &scene);

/// Given some random numbers, sample a point on the light source.
/// If the point is on a surface, returns both the point & normal on it.
/// If the point is infinitely far away (e.g., on an environment map),
/// we store the direction that points towards the origin in PointAndNormal.normal.
/// rnd_param_w is usually used for choosing a discrete element e.g., choosing a triangle in a mesh light.
/// rnd_param_uv is usually used for picking a point on that element.
PointAndNormal sample_point_on_light(const Light &light, 
                                     const Vector2 &rnd_param_uv,
                                     Real rnd_param_w,
                                     const Scene &scene);

/// Given a point on the light source, compute the sampling density for the function above.
Real pdf_point_on_light(const Light &light, const PointAndNormal &point_on_light, const Scene &scene);

/// Given a viewing direction pointing outwards from the light, and a point on the light,
/// compute the emission of the light. We also need the "footprint" of the ray
/// for texture filtering. For finite position, view_footprint stores (approximatedly) du/dx
/// and for infinite direction (e.g., envmap), view_footprint stores the approximated ddir/dx.
Spectrum emission(const Light &light,
                  const Vector3 &view_dir,
                  Real view_footprint,
                  const PointAndNormal &point_on_light,
                  const Scene &scene);

/// Some lights require storing sampling data structures inside. This function initialize them.
void init_sampling_dist(Light &light, const Scene &scene);
\end{lstlisting}

Hopefully the comments are self-explanatory (let us know if they are not). I recommend you read the source code of these lights to have a better understanding of what is happening.

\section{Geometry}
A renderer also needs representation of surfaces in the scene. Lajolla supports two kinds of geometry (or \lstinline{Shape}):

\begin{lstlisting}
struct Sphere : public ShapeBase {
    Vector3 position;
    Real radius;
};

struct TriangleMesh : public ShapeBase {
    std::vector<Vector3> positions;
    std::vector<Vector3i> indices;
    std::vector<Vector3> normals;
    std::vector<Vector2> uvs;
    /// Below are used only when the mesh is associated with an area light
    Real total_area;
    /// For sampling a triangle based on its area
    TableDist1D triangle_sampler;
};

using Shape = std::variant<Sphere, TriangleMesh>;
\end{lstlisting}

Lajolla uses \href{Embree}{https://www.embree.org/} as its ray casting engine. So a shape would need to register itself to the Embree API:
\begin{lstlisting}[language=c++]
/// Add the shape to an Embree scene.
uint32_t register_embree(const Shape &shape, const RTCDevice &device, const RTCScene &scene);
\end{lstlisting}

Embree has native support for a triangle mesh, so registering a triangle mesh to Embree is simple:
\begin{lstlisting}[language=c++]
uint32_t register_embree_op::operator()(const TriangleMesh &mesh) const {
    RTCGeometry rtc_geom = rtcNewGeometry(device, RTC_GEOMETRY_TYPE_TRIANGLE);
    // A geomID is the ID associated with the shape inside Embree.
    uint32_t geomID = rtcAttachGeometry(scene, rtc_geom);
    Vector4f *positions = (Vector4f*)rtcSetNewGeometryBuffer(
        rtc_geom, RTC_BUFFER_TYPE_VERTEX, 0, RTC_FORMAT_FLOAT3,
        sizeof(Vector4f), mesh.positions.size());
    Vector3i *triangles = (Vector3i*)rtcSetNewGeometryBuffer(
        rtc_geom, RTC_BUFFER_TYPE_INDEX, 0, RTC_FORMAT_UINT3,
        sizeof(Vector3i), mesh.indices.size());
    for (int i = 0; i < (int)mesh.positions.size(); i++) {
        Vector3 position = mesh.positions[i];
        positions[i] = Vector4f{(float)position[0], (float)position[1], (float)position[2], 0.f};
    }
    for (int i = 0; i < (int)mesh.indices.size(); i++) {
        triangles[i] = mesh.indices[i];
    }
    rtcSetGeometryVertexAttributeCount(rtc_geom, 1);
    rtcCommitGeometry(rtc_geom);
    rtcReleaseGeometry(rtc_geom);
    return geomID;
}
\end{lstlisting}

Registering a sphere is more complicated, since we will need to implement our own ray-sphere intersection routines:
\begin{lstlisting}[language=c++]
void sphere_bounds_func(const struct RTCBoundsFunctionArguments* args);
void sphere_intersect_func(const RTCIntersectFunctionNArguments* args);
void sphere_occluded_func(const RTCOccludedFunctionNArguments* args);

uint32_t register_embree_op::operator()(const Sphere &sphere) const {
    RTCGeometry rtc_geom = rtcNewGeometry(device, RTC_GEOMETRY_TYPE_USER);
    uint32_t geomID = rtcAttachGeometry(scene, rtc_geom);
    rtcSetGeometryUserPrimitiveCount(rtc_geom, 1);
    rtcSetGeometryUserData(rtc_geom, (void *)&sphere);
    rtcSetGeometryBoundsFunction(rtc_geom, sphere_bounds_func, nullptr);
    rtcSetGeometryIntersectFunction(rtc_geom, sphere_intersect_func);
    rtcSetGeometryOccludedFunction(rtc_geom, sphere_occluded_func);
    rtcCommitGeometry(rtc_geom);
    rtcReleaseGeometry(rtc_geom);
    return geomID;
}
\end{lstlisting}

Apart from the Embree registration, we also need to compute relevant information for shading and texture filtering.
\begin{lstlisting}[language=c++]
struct ShadingInfo {
    Vector2 uv; // UV coordinates for texture mapping
    Frame shading_frame; // the coordinate basis for shading
    Real mean_curvature; // 0.5 * (dot(dN/du, shading_frame.x) + dot(dN/dv, shading_frame.y))
    // Stores min(length(dp/du), length(dp/dv)), for ray differentials.
    Real inv_uv_size;
};

ShadingInfo compute_shading_info(const Shape &shape, const PathVertex &vertex);
\end{lstlisting}
All of these quantities can be computed using elementary differential geometry. You should read the source code if you are interested.

Finally, a shape also needs to implement some other sampling routines for area light sampling:
\begin{lstlisting}[language=c++]
/// Sample a point on the surface
PointAndNormal sample_point_on_shape(const Shape &shape, const Vector2 &uv, Real w);

/// Probability density of the operation above
Real pdf_point_on_shape(const Shape &shape);

/// Useful for sampling.
Real surface_area(const Shape &shape);

/// Some shapes require storing sampling data structures inside. This function initialize them.
void init_sampling_dist(Shape &shape);
\end{lstlisting}

\section{Utilities}

\subsection{Vectors}

\subsection{Matrices}

\subsection{Frame}

\subsection{Color}

\subsection{Parallelization}

\subsection{Random number generation}

\subsection{Russian roulette}

\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file

\end{document}
